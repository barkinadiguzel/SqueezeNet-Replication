# ğŸŒº SqueezeNet-Replication PyTorch Implementation

This repository contains a replication of **SqueezeNet** using PyTorch. The goal is to build a **lightweight CNN backbone** with **Fire modules** (squeeze + expand) for efficient inference and small model size.

- Implemented **SqueezeNet** with Fire modules (1Ã—1 squeeze + 1Ã—1 & 3Ã—3 expand).  
- Architecture:  
**Conv â†’ FireModule â†’ ... â†’ FireModule â†’ AvgPool â†’ Flatten â†’ FC**  
**Paper**: [SqueezeNet: AlexNet-level accuracy with 50x fewer parameters](https://arxiv.org/abs/1602.07360)

---

## ğŸ–¼ Overview â€“ SqueezeNet with Fire Modules

![Figure 1-2](images/figmix.jpg)  

**Figure 1 & Figure 2:** Sketch of SqueezeNet stages. Each stage contains multiple Fire modules. Squeeze layers reduce the number of input channels, and expand layers (1Ã—1 & 3Ã—3 convs) increase representational capacity.  

> **Model overview:**  
> SqueezeNet is a small, fully convolutional network designed for **minimal parameters** while retaining **AlexNet-level accuracy**. The model balances
> **compact size, speed, and efficiency** using fire modules and optional bypass connections for improved gradient flow.

---

## ğŸ— Project Structure

```bash
SqueezeNet-Replication/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ layers/
â”‚   â”‚   â”œâ”€â”€ conv_layer.py           # Standart conv layer (1x1, 3x3)
â”‚   â”‚   â”œâ”€â”€ fire_module.py          # Fire module (squeeze + expand)
â”‚   â”‚   â”œâ”€â”€ pool_layers/
â”‚   â”‚   â”‚   â”œâ”€â”€ maxpool_layer.py
â”‚   â”‚   â”‚   â””â”€â”€ avgpool_layer.py
â”‚   â”‚   â””â”€â”€ flatten_layer.py
â”‚   â”‚
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â””â”€â”€ squeezenet_model.py     # SqueezeNet assembly with Fire modules
â”‚   â”‚
â”‚   â””â”€â”€ config.py
â”œâ”€â”€ images/
â”‚   â””â”€â”€ figmix.jpg
â”‚ 
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

```
---

## ğŸ”— Feedback

For questions or feedback, contact: [barkin.adiguzel@gmail.com](mailto:barkin.adiguzel@gmail.com)
